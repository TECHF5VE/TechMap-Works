# readme

这一次的任务中时间拖得有点久，中间也遇到不少问题导致进度停滞。记录如下：

1、pycharm导入scrapy失败，升级pip百度改配置文件都没用。解决方案是下载了anaconda，用pycharm写py文件，用anaconda创建和执行爬虫

2、xpath学习用的是Firefox的一个插件，页面源代码和搜索框两头翻，废了不少劲。不过提取信息比之前的正则好用。现在翻了墙有了谷歌，有了xpath helper

3、一开始在主页面提取完名称、评分，进入详细页爬取简介时无法写入item，当时还不会用`"meta":item`传值，所以改为信息全部进入详细页提取。

4、可以说scrapy是多线程的，所以爬取页面的顺序不确定，导致影片乱序，百度同步爬取网上博客说的都和数据库什么的有关系？？设置自减优先级也没效果，存疑。再了解了解[Twisted框架]、同步异步I/O解决问题。

收获是对scrapy有了一个了解，熟悉了爬虫的一个基本架构和流程。会简单用xpath提取信息。

这次反爬只添加了一个浏览器请求头，未添加其他字段和其他反爬措施。还有就是这次任务从考试周前一直断断续续做到现在，应该加强效率。最后直到前几天我才知道爬取url是新片排行榜，当时只看到了排行榜.....所以搜了TOP250